# ğŸ† VoxCoder â€” Voice-Powered Code Agent

## Hackathon: Mistral Worldwide Hackathon 2026 (Online Edition)
## Track: "Tutto Ã¨ concesso" â€” Best demo using Mistral models via API/OSS
## Prize target: ğŸ¤ Best Voice Use Case (special prize)

---

## ğŸ¯ Concept (One-liner)

**Parli al microfono, un agente AI scrive codice per te in tempo reale.** Un pair-programmer vocale hands-free che combina Voxtral (speech-to-text), Mistral Agents API (orchestrazione + code execution), e una UI web live.

---

## ğŸ—ï¸ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     audio stream      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Browser     â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º   â”‚  Voxtral Transcribe â”‚
â”‚   (Mic input) â”‚                       â”‚  (mistral API)      â”‚
â”‚   + UI        â”‚ â—„â”€â”€â”€â”€ transcript â”€â”€â”€â”€ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚               â”‚                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚                                â”‚ transcript text
â”‚  â”‚Code Panelâ”‚ â”‚                                â–¼
â”‚  â”‚(live)    â”‚ â”‚                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â”‚          â”‚ â”‚ â—„â”€â”€ code + output â”€â”€â”€ â”‚  Mistral Agents API â”‚
â”‚  â”‚Terminal  â”‚ â”‚                       â”‚  (code_interpreter)  â”‚
â”‚  â”‚Output    â”‚ â”‚                       â”‚  model: mistral-     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚                       â”‚  large-latest        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Flusso:
1. L'utente parla nel microfono del browser
2. L'audio viene registrato in chunk e inviato a **Voxtral Mini Transcribe** via API
3. La trascrizione viene passata come messaggio a un **Mistral Agent** con `code_interpreter` attivo
4. L'Agent scrive codice Python, lo esegue nel sandbox, e restituisce risultato + codice
5. La UI mostra in tempo reale: trascrizione, codice generato, output dell'esecuzione

---

## ğŸ”§ Tech Stack

| Componente | Tecnologia | Motivo |
|---|---|---|
| **Frontend** | HTML/CSS/JS vanilla + WebSocket | Leggero, veloce da buildare. Usa `MediaRecorder` API per catturare audio dal mic |
| **Backend** | Python (FastAPI + WebSockets) | Gestisce il flusso audioâ†’trascrizioneâ†’agentâ†’risposta |
| **Speech-to-Text** | Mistral Voxtral API (`voxtral-mini-latest`) | Transcription endpoint, $0.003/min |
| **Agent + Code Exec** | Mistral Agents API (`mistral-large-latest`) con `code_interpreter` | Genera ed esegue codice Python in sandbox |
| **Deploy** | Hugging Face Spaces (Gradio/Docker) oppure locale | Per la demo hackathon |

---

## ğŸ“¦ Dependencies

```
# requirements.txt
mistralai>=1.0.0
fastapi>=0.100.0
uvicorn>=0.20.0
websockets>=12.0
python-dotenv>=1.0.0
```

---

## ğŸ”‘ Environment Variables

```bash
MISTRAL_API_KEY=your_mistral_api_key_here
```

---

## ğŸ“‹ Implementation Plan (Step-by-step)

### Step 1: Backend â€” FastAPI server con WebSocket

Crea `server.py`:

- Un endpoint WebSocket `/ws` che:
  - Riceve chunk audio (binary) dal browser
  - Li accumula in un buffer
  - Quando rileva silenzio (o riceve un segnale "end") invia l'audio a Voxtral per trascrizione
  - Invia la trascrizione come messaggio all'Agent
  - Streama indietro al client: trascrizione, codice generato, output esecuzione

### Step 2: Integrazione Voxtral Transcription API

Usa l'endpoint `/v1/audio/transcriptions` di Mistral:

```python
from mistralai import Mistral
import os

client = Mistral(api_key=os.environ["MISTRAL_API_KEY"])

# Trascrizione da file audio
transcription = client.audio.transcriptions.create(
    model="voxtral-mini-latest",
    file={
        "file_name": "recording.wav",
        "content": audio_bytes,  # bytes dell'audio registrato
    }
)
print(transcription.text)
```

**Parametri importanti:**
- `model`: `"voxtral-mini-latest"` (usa Voxtral Mini Transcribe, ottimizzato per trascrizione)
- Formati audio supportati: `.mp3`, `.wav`, `.m4a`, `.flac`, `.ogg`
- Max file size: 1GB
- Prezzo: $0.003/minuto
- Supporta `language` param per forzare la lingua (es. `"en"`, `"it"`)

### Step 3: Integrazione Mistral Agents API con Code Interpreter

Crea un agent con code execution:

```python
from mistralai import Mistral

client = Mistral(api_key=os.environ["MISTRAL_API_KEY"])

# Crea l'agent (una volta sola, salva l'agent_id)
code_agent = client.beta.agents.create(
    model="mistral-large-latest",
    name="VoxCoder",
    description="A voice-controlled coding assistant that writes and executes Python code.",
    instructions="""You are VoxCoder, a voice-controlled pair programmer. 
The user speaks commands to you via voice (transcribed to text).

Rules:
- When the user asks to write code, write clean Python code and execute it using the code interpreter.
- Always show the code you wrote AND the execution output.
- Keep responses concise â€” the user is speaking, not typing.
- If the user says something ambiguous, ask a SHORT clarifying question.
- Support iterative development: the user can say "fix that", "add error handling", "make it faster" etc.
- When the user says "save" or "export", output the final complete code.
- Respond in the same language the user speaks (Italian or English).
""",
    tools=[{"type": "code_interpreter"}],
    completion_args={
        "temperature": 0.3,
        "top_p": 0.9
    }
)

print(f"Agent ID: {code_agent.id}")
```

**Avviare una conversazione:**

```python
# Prima interazione
response = client.beta.conversations.start(
    agent_id=code_agent.id,
    inputs="Write a function to calculate fibonacci numbers and test it with n=10"
)

# Estrarre il risultato
for entry in response.outputs:
    if hasattr(entry, 'content'):
        print(entry.content)
```

**Continuare la conversazione (multi-turn):**

```python
# Turni successivi nella stessa conversazione
response = client.beta.conversations.append(
    conversation_id=response.conversation_id,
    inputs="Now make it use memoization for better performance"
)
```

**Costi:**
- `mistral-large-latest`: $2/M input tokens, $6/M output tokens
- `code_interpreter`: $0.03 per esecuzione
- `web_search` (opzionale): $0.03 per chiamata

### Step 4: Frontend â€” Browser UI

Crea `index.html` con:

**Layout a 3 pannelli:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   VoxCoder ğŸ¤                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  TRANSCRIPT    â”‚   CODE         â”‚   OUTPUT       â”‚
â”‚                â”‚                â”‚                â”‚
â”‚  "Write a      â”‚  def fib(n):   â”‚  >>> fib(10)   â”‚
â”‚   fibonacci    â”‚    if n <= 1:  â”‚  55            â”‚
â”‚   function..." â”‚      return n  â”‚                â”‚
â”‚                â”‚    return ...  â”‚  [Executed âœ“]  â”‚
â”‚                â”‚                â”‚                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  [ğŸ¤ Hold to Talk]  [â¹ Stop]  Status: Listening â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**FunzionalitÃ  JS del frontend:**

1. **Cattura audio dal microfono** usando `navigator.mediaDevices.getUserMedia()` e `MediaRecorder` API
2. **Push-to-talk** o **Voice Activity Detection (VAD)**: il modo piÃ¹ semplice Ã¨ push-to-talk (tieni premuto per parlare)
3. **Invio audio via WebSocket** al backend come binary blob
4. **Ricezione risposte** via WebSocket: trascrizione, codice, output â€” ciascuno renderizzato nel pannello corretto
5. **Syntax highlighting** per il codice: usa [Prism.js](https://prismjs.com/) o [highlight.js](https://highlightjs.org/) inline (CDN)

**Audio recording in JS:**
```javascript
let mediaRecorder;
let audioChunks = [];

async function startRecording() {
    const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
    mediaRecorder = new MediaRecorder(stream, { mimeType: 'audio/webm' });
    
    mediaRecorder.ondataavailable = (event) => {
        audioChunks.push(event.data);
    };
    
    mediaRecorder.onstop = async () => {
        const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
        audioChunks = [];
        // Invia al backend via WebSocket
        ws.send(audioBlob);
    };
    
    mediaRecorder.start();
}

function stopRecording() {
    mediaRecorder.stop();
}
```

### Step 5: WebSocket Protocol

Definisci un protocollo JSON semplice per i messaggi WebSocket:

**Client â†’ Server:**
```json
// Binary message: raw audio bytes (WAV/WebM)
```

**Server â†’ Client:**
```json
{
    "type": "transcript",
    "text": "Write a fibonacci function and test it"
}
```
```json
{
    "type": "code",
    "language": "python",
    "content": "def fibonacci(n):\n    if n <= 1:\n        return n\n    return fibonacci(n-1) + fibonacci(n-2)\n\nprint(fibonacci(10))"
}
```
```json
{
    "type": "output",
    "content": "55"
}
```
```json
{
    "type": "status",
    "status": "transcribing" | "thinking" | "executing" | "ready" | "error",
    "message": "optional error message"
}
```

### Step 6: Conversione audio (importante!)

Il browser registra in WebM/Opus. L'API Voxtral accetta WAV, MP3, FLAC, OGG, M4A.

Opzione piÃ¹ semplice: converti server-side con `ffmpeg`:

```python
import subprocess
import tempfile

def convert_webm_to_wav(webm_bytes: bytes) -> bytes:
    with tempfile.NamedTemporaryFile(suffix='.webm', delete=False) as f_in:
        f_in.write(webm_bytes)
        f_in_path = f_in.name
    
    f_out_path = f_in_path.replace('.webm', '.wav')
    
    subprocess.run([
        'ffmpeg', '-i', f_in_path,
        '-ar', '16000',  # 16kHz sample rate
        '-ac', '1',       # mono
        '-f', 'wav',
        f_out_path
    ], capture_output=True)
    
    with open(f_out_path, 'rb') as f_out:
        return f_out.read()
```

**NOTA:** Assicurati che `ffmpeg` sia installato nell'ambiente. Su HF Spaces Docker Ã¨ disponibile.

---

## ğŸ¨ UI Design Guidelines

- **Tema scuro** (stile terminale/IDE) â€” piÃ¹ "developer"
- **Font monospace** per codice (Fira Code, JetBrains Mono via Google Fonts)
- **Animazioni minime** ma impattanti:
  - Pulsing glow sul bottone mic quando attivo
  - Typing effect per la trascrizione
  - Codice che appare riga per riga
- **Colori**: sfondo `#0d1117` (GitHub dark), accent `#f97316` (arancio Mistral), codice `#22c55e` (verde terminale)
- **Logo/brand**: "VoxCoder" con icona microfono + code brackets `{ğŸ¤}`

---

## ğŸš€ MVP Features (must-have per la demo)

1. âœ… Push-to-talk: tieni premuto il bottone, parla, rilascia
2. âœ… Trascrizione live mostrata nella UI
3. âœ… Codice Python generato e mostrato con syntax highlighting
4. âœ… Esecuzione del codice nel sandbox e output mostrato
5. âœ… Multi-turn: puoi dire "ora aggiungi error handling" e il contesto Ã¨ mantenuto
6. âœ… Indicatori di stato (transcribing â†’ thinking â†’ executing â†’ ready)

## âœ¨ Nice-to-have (se c'Ã¨ tempo)

- ğŸ”Š Text-to-Speech per le risposte dell'agent (ElevenLabs API â€” c'Ã¨ un premio speciale ElevenLabs!)
- ğŸ“‹ Bottone "Copy Code" e "Download .py"  
- ğŸŒ Web search integration per domande tipo "trova la libreria migliore per X"
- ğŸ“Š Visualizzazione inline di grafici matplotlib (l'agent puÃ² generare immagini)
- ğŸ—‚ï¸ Cronologia dei comandi vocali con possibilitÃ  di replay

---

## ğŸ“ Project Structure

```
voxcoder/
â”œâ”€â”€ server.py              # FastAPI backend + WebSocket handler
â”œâ”€â”€ agent.py               # Mistral Agent creation & conversation logic
â”œâ”€â”€ transcriber.py         # Voxtral transcription wrapper
â”œâ”€â”€ audio_utils.py         # Audio conversion (webmâ†’wav via ffmpeg)
â”œâ”€â”€ static/
â”‚   â”œâ”€â”€ index.html         # Main UI page
â”‚   â”œâ”€â”€ style.css          # Dark theme styling
â”‚   â”œâ”€â”€ app.js             # Frontend logic (mic, websocket, rendering)
â”‚   â””â”€â”€ prism.js           # Syntax highlighting (bundle from CDN)
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile             # Per HF Spaces deploy
â”œâ”€â”€ README.md              # Project description per hackathon submission
â””â”€â”€ .env.example
```

---

## ğŸ³ Dockerfile (per HF Spaces)

```dockerfile
FROM python:3.11-slim

RUN apt-get update && apt-get install -y ffmpeg && rm -rf /var/lib/apt/lists/*

WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

EXPOSE 7860

CMD ["uvicorn", "server:app", "--host", "0.0.0.0", "--port", "7860"]
```

---

## ğŸ“Š API Cost Estimate (per la demo)

| Servizio | Uso stimato | Costo |
|---|---|---|
| Voxtral Transcribe | ~30 min totali di audio | ~$0.09 |
| Mistral Large (agent) | ~50 interazioni | ~$0.50 |
| Code Interpreter | ~50 esecuzioni | ~$1.50 |
| **Totale demo** | | **~$2.10** |

I crediti dell'hackathon coprono ampiamente.

---

## ğŸ Demo Script (2 minuti per il video)

1. **[0:00-0:15]** "Hi, I'm presenting VoxCoder â€” a voice-controlled coding assistant powered entirely by Mistral AI."
2. **[0:15-0:45]** Primo comando: "Create a function that takes a list of numbers and returns the mean, median, and standard deviation" â†’ mostra codice generato + esecuzione
3. **[0:45-1:15]** Follow-up vocale: "Now add a visualization â€” plot a histogram of a random sample using matplotlib" â†’ mostra il grafico inline
4. **[1:15-1:40]** "Add error handling for empty lists and non-numeric values" â†’ mostra il codice aggiornato
5. **[1:40-2:00]** "VoxCoder combines Voxtral for voice, the Agents API for orchestration, and Code Interpreter for execution â€” all Mistral. Thanks!"

---

## âš ï¸ Gotchas & Tips

1. **CORS**: FastAPI deve servire i file statici E il WebSocket dallo stesso server per evitare problemi CORS con il microfono
2. **HTTPS**: `getUserMedia()` richiede HTTPS (o localhost). HF Spaces fornisce HTTPS automaticamente.
3. **Audio format**: Il browser registra in WebM/Opus. DEVI convertire a WAV prima di inviare a Voxtral.
4. **Agent persistence**: Crea l'agent UNA volta all'avvio del server e riusa l'`agent_id`. Crea una nuova `conversation_id` per ogni sessione utente.
5. **Rate limits**: L'API Voxtral ha limiti. Non inviare chunk troppo piccoli â€” accumula almeno 2-3 secondi di audio prima di trascrivere.
6. **Timeout**: Il code_interpreter puÃ² impiegare qualche secondo. Mostra un indicatore di loading nella UI.
7. **Modelli supportati per Agents API**: attualmente `mistral-medium-latest` e `mistral-large-latest`. DevStral NON Ã¨ ancora supportato direttamente nell'Agents API (ma il code_interpreter Ã¨ built-in e funziona con Large/Medium).

---

## ğŸ“š Reference Links

- Mistral Python SDK: https://github.com/mistralai/client-python
- Voxtral Transcription Docs: https://docs.mistral.ai/capabilities/audio_transcription
- Voxtral API Endpoint: https://docs.mistral.ai/api/endpoint/audio/transcriptions
- Agents API Introduction: https://docs.mistral.ai/agents/introduction
- Agents & Conversations: https://docs.mistral.ai/agents/agents
- Agents API Blog: https://mistral.ai/news/agents-api
- Code Interpreter Tool: type `code_interpreter` nel tools array dell'agent
- Hackathon Page: https://huggingface.co/mistral-hackaton-2026
- Hackathon Platform: https://hackiterate.com
- Discord: https://discord.gg/zdSEmdfkSQ